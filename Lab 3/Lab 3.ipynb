{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Extending Logistic Regression\n",
    "\n",
    "#### *Harrison Noble & Henry Lambson*\n",
    "\n",
    "## 1. Preparation & Overview\n",
    "\n",
    "### 1.1 Business Understanding\n",
    "\n",
    "In this lab we will be using the same wine quality dataset [Ref 1] we used in Lab 1 [Ref 2]. To jog your memory, the wine quality dataset contains 1599 red wine samples and 4898 white wine samples, with 6497 samples in total. There are 12 unique features including the quality value. Each feature is aside from the quality is numerical, with the quality being categorical.\n",
    "\n",
    "The task for this lab is to create custom logistic regression classifiers and optimization techniques for predicting the quality of both red and white wines based on their physical/chemical attributes. The use-case for a classifier of this nature would be to create an initial screening process for new wines to help wine tasters filter out high quality wines. This would allow wine testers to only test high quality wines that made it through our filter in order to save time and resources that would have been spent on lower quality wines not fit for market. __To create an effective wine quality classifier which could eventually be used as a filtering software, we would like to have a prediction accuracy of 90% or higher that minimizes false positives. Minimizing false positives is ideal for this task so that lower quality wines are more likely to get screened out and only high quality wines make it through. This would lead wine testers/tasters to have a higher level of trust in our algorithm.__ \n",
    "\n",
    "__The model created from this dataset would mostly used for offline analysis.__ Because wine testers/tasters do not necessarily need to know the predicted quality of the wine that is produced instantly, this model can be used for offline analysis. One potential application of the model would be doing a weekly analysis of all wines submitted during that week. In this case, wine testers/tasters would gain insights on the high quality wines to potentially test further. \n",
    "\n",
    "One change we are making for this lab is keeping the white and red wine datasets seperate. Based on our analysis from Lab 1, we found that some attribute values that make white wine good would make red wine worse and vice versa. Therefore, we will train and test our regression models on each dataset seperately to see if the model works better on one or the other (or both!)\n",
    "\n",
    "### 1.2 Defining and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of red wine dataset: 1599\n",
      "Length of white wine dataset: 4898\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#This code chunk was taken from our Lab 1 assignment and edited for this lab.\n",
    "# load red and white wine datasets into pandas\n",
    "df_red = pd.read_csv('./winequality-red.csv', sep=';')\n",
    "df_white = pd.read_csv('./winequality-white.csv', sep=';')\n",
    "\n",
    "#print number of rows in each df to confirm all data is loaded in\n",
    "print('Length of red wine dataset:', df_red.shape[0])\n",
    "print('Length of white wine dataset:', df_white.shape[0])\n",
    "\n",
    "df_red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just double checking the data was loaded into the dataframes correctly. One thing we are doing different in this lab is keeping the white and red wines separate. This was stated in section 1.1 but we just wanted to clarify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of red wine dataset: 1458\n",
      "Length of white wine dataset: 4502\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.312551</td>\n",
       "      <td>0.524050</td>\n",
       "      <td>0.265281</td>\n",
       "      <td>2.388717</td>\n",
       "      <td>0.081531</td>\n",
       "      <td>15.089849</td>\n",
       "      <td>43.660494</td>\n",
       "      <td>0.996718</td>\n",
       "      <td>3.316152</td>\n",
       "      <td>0.642414</td>\n",
       "      <td>10.417798</td>\n",
       "      <td>5.646776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.647635</td>\n",
       "      <td>0.169451</td>\n",
       "      <td>0.191271</td>\n",
       "      <td>0.865307</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>9.317669</td>\n",
       "      <td>29.414615</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.141052</td>\n",
       "      <td>0.129753</td>\n",
       "      <td>1.021649</td>\n",
       "      <td>0.801119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.991500</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>3.315000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.500000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>1.002200</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1458.000000       1458.000000  1458.000000     1458.000000   \n",
       "mean        8.312551          0.524050     0.265281        2.388717   \n",
       "std         1.647635          0.169451     0.191271        0.865307   \n",
       "min         5.000000          0.120000     0.000000        1.200000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.250000        2.200000   \n",
       "75%         9.200000          0.635000     0.420000        2.600000   \n",
       "max        13.500000          1.040000     0.790000        6.700000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1458.000000          1458.000000           1458.000000  1458.000000   \n",
       "mean      0.081531            15.089849             43.660494     0.996718   \n",
       "std       0.021218             9.317669             29.414615     0.001718   \n",
       "min       0.038000             1.000000              6.000000     0.991500   \n",
       "25%       0.070000             7.000000             21.000000     0.995600   \n",
       "50%       0.079000            13.000000             36.000000     0.996700   \n",
       "75%       0.089000            21.000000             58.000000     0.997800   \n",
       "max       0.226000            47.000000            145.000000     1.002200   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1458.000000  1458.000000  1458.000000  1458.000000  \n",
       "mean      3.316152     0.642414    10.417798     5.646776  \n",
       "std       0.141052     0.129753     1.021649     0.801119  \n",
       "min       2.880000     0.330000     8.400000     3.000000  \n",
       "25%       3.220000     0.550000     9.500000     5.000000  \n",
       "50%       3.315000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.720000    11.100000     6.000000  \n",
       "max       3.750000     1.160000    13.600000     8.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "to_use = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "          'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "          'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "#remove outliers with absolute value z-scores 3 or higher\n",
    "df_red = df_red[(np.abs(stats.zscore(df_red[to_use])) < 3).all(axis=1)]\n",
    "df_white = df_white[(np.abs(stats.zscore(df_white[to_use])) < 3).all(axis=1)]\n",
    "\n",
    "print('Length of red wine dataset:', df_red.shape[0])\n",
    "print('Length of white wine dataset:', df_white.shape[0])\n",
    "\n",
    "df_red.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "      <td>4502.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.840749</td>\n",
       "      <td>0.271465</td>\n",
       "      <td>0.326513</td>\n",
       "      <td>6.414705</td>\n",
       "      <td>0.043153</td>\n",
       "      <td>34.821302</td>\n",
       "      <td>137.533319</td>\n",
       "      <td>0.993964</td>\n",
       "      <td>3.188256</td>\n",
       "      <td>0.485426</td>\n",
       "      <td>10.540115</td>\n",
       "      <td>5.912261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.086082</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>4.954158</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>15.427580</td>\n",
       "      <td>41.323011</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.143456</td>\n",
       "      <td>0.105782</td>\n",
       "      <td>1.226173</td>\n",
       "      <td>0.869685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.762500</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991663</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>1.001960</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4502.000000       4502.000000  4502.000000     4502.000000   \n",
       "mean        6.840749          0.271465     0.326513        6.414705   \n",
       "std         0.786885          0.086082     0.101038        4.954158   \n",
       "min         4.400000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.762500   \n",
       "50%         6.800000          0.260000     0.310000        5.300000   \n",
       "75%         7.300000          0.320000     0.380000        9.900000   \n",
       "max         9.300000          0.580000     0.690000       20.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4502.000000          4502.000000           4502.000000  4502.000000   \n",
       "mean      0.043153            34.821302            137.533319     0.993964   \n",
       "std       0.011724            15.427580             41.323011     0.002908   \n",
       "min       0.012000             2.000000             19.000000     0.987110   \n",
       "25%       0.035000            23.000000            108.000000     0.991663   \n",
       "50%       0.042500            34.000000            133.000000     0.993700   \n",
       "75%       0.050000            45.000000            166.000000     0.996100   \n",
       "max       0.110000            86.000000            260.000000     1.001960   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4502.000000  4502.000000  4502.000000  4502.000000  \n",
       "mean      3.188256     0.485426    10.540115     5.912261  \n",
       "std       0.143456     0.105782     1.226173     0.869685  \n",
       "min       2.790000     0.220000     8.400000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.540000    11.400000     6.000000  \n",
       "max       3.640000     0.830000    14.200000     9.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_white.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our dataset has no outlier values after removing values with an absolute value z-score greater than or equal to 3. In total, there are now 1458 instances in the red wine dataset and 4502 instances in the white wine dataset. We also know that there are no missing values in either of these datasets from our analysis in Lab 1. We can now remove the quality and store it in a variable called \"y\" for each respective wine color. This process is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract prediction task (quality) from datasets and then remove\n",
    "y_red = df_red['quality']\n",
    "X_red = df_red.drop(columns='quality')\n",
    "y_white = df_white['quality']\n",
    "X_white = df_white.drop(columns='quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Description</th>\n",
       "      <th>Scales</th>\n",
       "      <th>Discrete\\Continuous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed acidity</td>\n",
       "      <td>grams of tartaric acid per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>volatile acidity</td>\n",
       "      <td>grams of acetic acid per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>citric acid</td>\n",
       "      <td>grams of citric acid per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residual sugar</td>\n",
       "      <td>grams of sugar per liter remaining after fermentation stops</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chlorides</td>\n",
       "      <td>grams of sodium chloride (salt) per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>free sulfur dioxide</td>\n",
       "      <td>milligrams of free form sulfur dioxide per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total sulfur dioxide</td>\n",
       "      <td>milligrams of free form and bound forms of sulfur dioxide per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>density</td>\n",
       "      <td>density of the wine (grams per cubic centimeter)</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pH</td>\n",
       "      <td>the pH value of the wine</td>\n",
       "      <td>interval</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sulphates</td>\n",
       "      <td>grams of potassium sulphate per liter</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>percent of alcohol by volume</td>\n",
       "      <td>ratio</td>\n",
       "      <td>Continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>quality</td>\n",
       "      <td>median score given by wine tasters (prediction task)</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>Discrete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Attributes  \\\n",
       "0          fixed acidity   \n",
       "1       volatile acidity   \n",
       "2            citric acid   \n",
       "3         residual sugar   \n",
       "4              chlorides   \n",
       "5    free sulfur dioxide   \n",
       "6   total sulfur dioxide   \n",
       "7                density   \n",
       "8                     pH   \n",
       "9              sulphates   \n",
       "10               alcohol   \n",
       "11               quality   \n",
       "\n",
       "                                                            Description  \\\n",
       "0                                      grams of tartaric acid per liter   \n",
       "1                                        grams of acetic acid per liter   \n",
       "2                                        grams of citric acid per liter   \n",
       "3           grams of sugar per liter remaining after fermentation stops   \n",
       "4                             grams of sodium chloride (salt) per liter   \n",
       "5                      milligrams of free form sulfur dioxide per liter   \n",
       "6   milligrams of free form and bound forms of sulfur dioxide per liter   \n",
       "7                      density of the wine (grams per cubic centimeter)   \n",
       "8                                              the pH value of the wine   \n",
       "9                                 grams of potassium sulphate per liter   \n",
       "10                                         percent of alcohol by volume   \n",
       "11                 median score given by wine tasters (prediction task)   \n",
       "\n",
       "      Scales Discrete\\Continuous  \n",
       "0      ratio          Continuous  \n",
       "1      ratio          Continuous  \n",
       "2      ratio          Continuous  \n",
       "3      ratio          Continuous  \n",
       "4      ratio          Continuous  \n",
       "5      ratio          Continuous  \n",
       "6      ratio          Continuous  \n",
       "7      ratio          Continuous  \n",
       "8   interval          Continuous  \n",
       "9      ratio          Continuous  \n",
       "10     ratio          Continuous  \n",
       "11   ordinal            Discrete  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data description dataframe, used in Lab 1\n",
    "description_df = pd.DataFrame()\n",
    "\n",
    "#df_white could also be used here since they have the same attributes\n",
    "description_df['Attributes'] = df_red.columns\n",
    "\n",
    "#Description of each attribute\n",
    "description_df['Description'] = ['grams of tartaric acid per liter', \n",
    "                                 'grams of acetic acid per liter',\n",
    "                                 'grams of citric acid per liter',\n",
    "                                 'grams of sugar per liter remaining after fermentation stops',\n",
    "                                 'grams of sodium chloride (salt) per liter',\n",
    "                                 'milligrams of free form sulfur dioxide per liter',\n",
    "                                 'milligrams of free form and bound forms of sulfur dioxide per liter',\n",
    "                                 'density of the wine (grams per cubic centimeter)',\n",
    "                                 'the pH value of the wine',\n",
    "                                 'grams of potassium sulphate per liter',\n",
    "                                 'percent of alcohol by volume',\n",
    "                                 'median score given by wine tasters (prediction task)']\n",
    "\n",
    "description_df['Scales'] = ['ratio', 'ratio', 'ratio', 'ratio', \n",
    "                            'ratio', 'ratio', 'ratio', 'ratio', \n",
    "                            'interval', 'ratio', 'ratio', 'ordinal']\n",
    "\n",
    "description_df['Discrete\\Continuous'] = ['Continuous', 'Continuous', 'Continuous',\n",
    "                                        'Continuous', 'Continuous', 'Continuous',\n",
    "                                        'Continuous', 'Continuous', 'Continuous',\n",
    "                                        'Continuous', 'Continuous', 'Discrete']\n",
    "\n",
    "#this display option found at: \n",
    "#https://stackoverflow.com/questions/25351968/how-to-display-full-non-truncated-dataframe-information-in-html-when-convertin\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "description_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ this table/code was taken from our lab 1 assignment and slightly edited for our needs in this lab. \n",
    "\n",
    "The above table gives us a description of each feature, data type (scales), and whether it is discrete or continuous. It should be noted that the \"quality\" attribute is not actually in the \"X_red\" or \"X_white\", but we included it for clarity. This is the final dataset we will be using for our regression tasks. Both white and red wine datasets contain these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Split Data Into Training & Testing Sets (80/20 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#perform 80/20 split on both datasets\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = tts(X_red, y_red, test_size=0.2)\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = tts(X_white, y_white, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe an 80/20 train-test split is a viable option for our dataset because of the size. We consider our datasets relatively small since our processed red wine dataset only contains 1458 instances and our processed white wine dataset contains 4502 instances. Because of this, we need as much training data as possible with a reasonable amount to test on. If our testing set was too small, we do not think the accuracy score of the model would truly represent the performance of the model. \n",
    "\n",
    "Another option to split data is dividing it into training, validation, and testing. Like our previous arguement, we do not think our datasets are large enough to perform this type of split. If we had tens to hundreds of thousands of instances, this type of split would be a possbility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "  \n",
    "- [1.5 points] Train your classifier to achieve good generalization performance. That is, adjust the __optimization technique__ and the value of the __regularization term \"C\"__ to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "- [1.5 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. __Discuss the results.__\n",
    "\n",
    "### 2.1 Create Custom One Vs. All Logistic Regression Classifier\n",
    "\n",
    "#### 2.1.1 Create BinaryLogisticRegression Class Using Example From Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code taken from 06. Optimization notebook\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    #extend init function by adding regularization term\n",
    "    def __init__(self, eta, iterations=20, C=0.001, regularization_term='none'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.reg_term = regularization_term\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    ################### private ###################\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    #Implement _get_gradient function in sub classes!\n",
    "    \n",
    "    ################### public ###################\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Create SteepestDescentBLR Class Using Example From Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteepestDescentBLR(BinaryLogisticRegression):\n",
    "    \n",
    "    # steepest descent calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if self.reg_term == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        if self.reg_term == 'L1':\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        if self.reg_term == 'both':\n",
    "            gradient[1:] += (np.sign(self.w_[1:]) * self.C) + (-2 * self.w_[1:] * self.C)\n",
    "        \n",
    "        #if reg_term is 'none' no need to do anything, just return gradient\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Create StochasticGradientDescentBLR Class Using Example From Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescentBLR(BinaryLogisticRegression):\n",
    "    \n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if self.reg_term == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        if self.reg_term == 'L1':\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        if self.reg_term == 'both':\n",
    "            gradient[1:] += (np.sign(self.w_[1:]) * self.C) + (-2 * self.w_[1:] * self.C)\n",
    "        \n",
    "        #if reg_term is 'none' no need to do anything, just return gradient\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Create NetwtonsMethodBLR Class Using Example From Class (Hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "\n",
    "class NewtonsMethodBLR(BinaryLogisticRegression):\n",
    "    \n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if self.reg_term == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        if self.reg_term == 'L1':\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        if self.reg_term == 'both':\n",
    "            gradient[1:] += (np.sign(self.w_[1:]) * self.C) + (-2 * self.w_[1:] * self.C)\n",
    "        \n",
    "        #if reg_term is 'none' no need to do anything, just return gradient\n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 Create Multiclass Logistic Regression Class Using Example From Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=SteepestDescentBLR,\n",
    "                 regularization_term='none'): #regularization_term options: 'none', 'L1', 'L2', 'both'\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        #store regularization term\n",
    "        self.reg_term = regularization_term\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            #add regularization term to solver params\n",
    "            blr = self.solver(eta=self.eta,\n",
    "                              iterations=self.iters,\n",
    "                              C=self.C,\n",
    "                              regularization_term=self.reg_term)\n",
    "            blr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle creating a custom one-versus-all logistic regression classifier with the ability to choose different optimization techniques and custom regularizations, we first began with the code provided in notebook 06. optimization. We used the ```BinaryLogisticRegression``` class as our base class and extended it with the ```StochasticLogisticRegression``` (renamed to ```StochasticGradientDescentBLR```), ```HessianBinaryLogisticRegression``` (renamed to ```NewtonsMethodBLR```), and the ```MultiClassLogisticRegression``` classes. Our ```SteepestDescentBLR``` class uses the ```_get_gradient``` function from the base ```BinaryLogisticRegression``` class. We decided to remove the ```_get_gradient``` function from the base class as it is implemented in the ```SteepestDescentBLR``` class. In each of the ```BinaryLogisticRegression``` child classes, the only additional/changed function is the ```_get_gradient``` function which calculates the gradient based on the given optimization technique. \n",
    "\n",
    "In regards to adding the ability to choose different optimization techniques, we stuck with your implementation which allowed the classifier to be passed in via a parameter. To add the ability to change normalization term we added the ```regularization_term``` parameter to the base ```BinaryLogisticRegression``` class and the ```MultiClassLogisticRegression``` class. The value for this parameter can either be \"L1\", \"L2\", \"both\", or \"none\" which specifies the type of regularization to use. To implement the different regularizations, we added a chain of if statements in each child class's ```_get_gradient``` function. If the regularization term chosen was either \"L1\", \"L2\", or \"both\", the gradient was adjusted according to the regularization. If the regularization term was \"none\", the gradients were not adjusted and simply returned as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train Classifier to Acheive Good Generalization Performance\n",
    "\n",
    "#### 2.2.1 Achieve Good Generalization Performance on Red Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy for red wine dataset is: 0.2534246575342466\n",
      "Best parameters are: {'Solver': <class '__main__.StochasticGradientDescentBLR'>, 'Eta': 0.1, 'C': 0.1, 'Regularization': 'L2'}\n",
      "Best model took 24.33 ms to fit and predict\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "solvers = [SteepestDescentBLR, StochasticGradientDescentBLR, NewtonsMethodBLR]\n",
    "etas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "regularization_terms = ['none', 'L1', 'L2', 'both']\n",
    "\n",
    "#use various parameters to predict red wine dataset\n",
    "best_acc_r = 0\n",
    "best_params_r = {}\n",
    "timing_r = 0\n",
    "for solver in solvers:\n",
    "    for eta in etas:\n",
    "        for c in Cs:\n",
    "            for reg_term in regularization_terms:\n",
    "                #time each iteration\n",
    "                t0 = time.time()\n",
    "                #create the Multiclass Logistic Regression object, train, and test\n",
    "                mclr = MultiClassLogisticRegression(eta=eta,\n",
    "                                                    iterations=50,\n",
    "                                                    C=c,\n",
    "                                                    solver=solver,\n",
    "                                                    regularization_term=reg_term)\n",
    "                mclr.fit(X_train_r, y_train_r)\n",
    "                mclr_yhat = mclr.predict(X_test_r)\n",
    "                t1 = time.time()\n",
    "\n",
    "                time_ms = (t1 - t0) * 1000\n",
    "                acc = accuracy_score(y_test_r, mclr_yhat)\n",
    "                \n",
    "                if acc > best_acc_r:\n",
    "                    best_acc_r = acc\n",
    "                    best_params_r = {'Solver': solver,\n",
    "                                     'Eta': eta,\n",
    "                                     'C':c,\n",
    "                                     'Regularization':reg_term}\n",
    "                    timing_r = time_ms\n",
    "\n",
    "print('Best model accuracy for red wine dataset is:', best_acc_r)\n",
    "print('Best parameters are:', best_params_r)\n",
    "print('Best model took %.2f ms to fit and predict' % timing_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best model for our red wine dataset has an accuracy score of around 25% in 24 milliseconds. It achieves this score by using the stochastic gradient descent optimazation, an eta value of 0.1, a C value of 0.1, and L2 regularization. In addition, this model was run with 50 iterations. Lets see if using these parameters and changing the number of iterations has any effect on the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations        Accuracy Time (ms)\n",
      "        10 0.0000000000000 29.69\n",
      "        20 0.0000000000000 44.94\n",
      "        50 0.0102739726027 59.14\n",
      "       100 0.4486301369863 57.97\n",
      "       150 0.0102739726027 86.23\n",
      "       200 0.0034246575342 93.89\n",
      "       250 0.0000000000000 99.96\n",
      "       300 0.0000000000000 159.74\n",
      "       350 0.0068493150685 141.35\n",
      "       400 0.0034246575342 441.47\n"
     ]
    }
   ],
   "source": [
    "iters = [10, 20, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "print('%10s %15s %8s' % ('Iterations', 'Accuracy', 'Time (ms)'))\n",
    "for i in iters:\n",
    "    #time each iteration\n",
    "    t0 = time.time()\n",
    "    #create the Multiclass Logistic Regression object, train, and test\n",
    "    mclr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                        iterations=i,\n",
    "                                        C=0.1,\n",
    "                                        solver=StochasticGradientDescentBLR,\n",
    "                                        regularization_term='L2')\n",
    "    mclr.fit(X_train_r, y_train_r)\n",
    "    mclr_yhat = mclr.predict(X_test_r)\n",
    "    t1 = time.time()\n",
    "\n",
    "    time_ms = (t1 - t0) * 1000\n",
    "    acc = accuracy_score(y_test_r, mclr_yhat)\n",
    "    \n",
    "    print('%10d %0.13f %.2f' % (i, acc, time_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Using 100 iterations this model was able to achieve 44% accuracy with a runtime of about 58 milliseconds. Lets visualize this models decision boundaries. To do this we will run PCA on our dataset to reduce it down to 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X_train_r)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#copied from notebook 05. logistic regression\n",
    "def plot_decision_boundaries(lr,Xin,y,title=''):\n",
    "    Xb = copy.deepcopy(Xin)\n",
    "    lr.fit(Xb[:,:2],y) # train only on two features\n",
    "\n",
    "    h=0.01\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "    y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # get prediction values\n",
    "    Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(Xb[:, 0], Xb[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "mclr = MultiClassLogisticRegression(eta=0.1,\n",
    "                                    iterations=100,\n",
    "                                    C=0.1,\n",
    "                                    solver=StochasticGradientDescentBLR,\n",
    "                                    regularization_term='L2')\n",
    "\n",
    "plot_decision_boundaries(mclr,X_tsne,y_train_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Achieve Good Generalization Performance on White Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy for white wine dataset is: 0.42508324084350724\n",
      "Best parameters are: {'Solver': <class '__main__.StochasticGradientDescentBLR'>, 'Eta': 0.0001, 'C': 0.1, 'Regularization': 'both'}\n",
      "Best model took 28.91 ms to fit and predict\n"
     ]
    }
   ],
   "source": [
    "#repeat same process but with white wine dataset\n",
    "best_acc_w = 0\n",
    "best_params_w = {}\n",
    "timing_w = 0\n",
    "for solver in solvers:\n",
    "    for eta in etas:\n",
    "        for c in Cs:\n",
    "            for reg_term in regularization_terms:\n",
    "                #time each iteration\n",
    "                t0 = time.time()\n",
    "                #create the Multiclass Logistic Regression object, train, and test\n",
    "                mclr = MultiClassLogisticRegression(eta=1,\n",
    "                                                    iterations=50,\n",
    "                                                    C=c,\n",
    "                                                    solver=solver,\n",
    "                                                    regularization_term=reg_term)\n",
    "                mclr.fit(X_train_w, y_train_w)\n",
    "                mclr_yhat = mclr.predict(X_test_w)\n",
    "                t1 = time.time()\n",
    "\n",
    "                time_ms = (t1 - t0) * 1000\n",
    "                acc = accuracy_score(y_test_w, mclr_yhat)\n",
    "\n",
    "                if acc > best_acc_w:\n",
    "                    best_acc_w = acc\n",
    "                    best_params_w = {'Solver': solver,\n",
    "                                     'Eta': eta,\n",
    "                                     'C':c,\n",
    "                                     'Regularization':reg_term}\n",
    "                    timing_w = time_ms\n",
    "\n",
    "print('Best model accuracy for white wine dataset is:', best_acc_w)\n",
    "print('Best parameters are:', best_params_w)\n",
    "print('Best model took %.2f ms to fit and predict' % timing_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best model for our white wine dataset has an accuracy score of around 42% in 28 milliseconds. It achieves this score by using the stochastic gradient descent optimazation, an eta value of 0.0001, a C value of 0.1, and both L1 and L2 regularizations. In addition, this model was run with 50 iterations. Lets see if using these parameters and changing the number of iterations has any effect on the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations        Accuracy Time (ms)\n",
      "        10 0.0000000000000 22.31\n",
      "        20 0.0000000000000 21.46\n",
      "        50 0.0000000000000 29.14\n",
      "       100 0.0000000000000 38.64\n",
      "       150 0.0000000000000 54.85\n",
      "       200 0.0000000000000 92.95\n",
      "       250 0.0188679245283 89.99\n",
      "       300 0.0144284128746 107.19\n",
      "       350 0.0011098779134 176.94\n",
      "       400 0.0000000000000 290.06\n"
     ]
    }
   ],
   "source": [
    "iters = [10, 20, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "print('%10s %15s %8s' % ('Iterations', 'Accuracy', 'Time (ms)'))\n",
    "for i in iters:\n",
    "    #time each iteration\n",
    "    t0 = time.time()\n",
    "    #create the Multiclass Logistic Regression object, train, and test\n",
    "    mclr = MultiClassLogisticRegression(eta=0.0001,\n",
    "                                        iterations=i,\n",
    "                                        C=0.1,\n",
    "                                        solver=StochasticGradientDescentBLR,\n",
    "                                        regularization_term='both')\n",
    "    mclr.fit(X_train_w, y_train_w)\n",
    "    mclr_yhat = mclr.predict(X_test_w)\n",
    "    t1 = time.time()\n",
    "\n",
    "    time_ms = (t1 - t0) * 1000\n",
    "    acc = accuracy_score(y_test_w, mclr_yhat)\n",
    "    \n",
    "    print('%10d %0.13f %.2f' % (i, acc, time_ms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing Our Optimization Performance to The Godly Scikit-Learn Optimization\n",
    "\n",
    "#### 2.3.1 Comparing Scikit-Learn Optimization on Red Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciKit-Learn accuracy score on red wine test dataset: 0.6095890410958904\n",
      "Total execution time: 40.13 milliseconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#test sklearn on red wine dataset\n",
    "t0 = time.time()\n",
    "skl_logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "skl_logreg.fit(X_train_r, y_train_r)\n",
    "skl_yhat = skl_logreg.predict(X_test_r)\n",
    "t1 = time.time()\n",
    "\n",
    "print('SciKit-Learn accuracy score on red wine test dataset:', \n",
    "      accuracy_score(y_test_r, skl_yhat))\n",
    "print('Total execution time: %.2f milliseconds' % ((t1-t0) * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Comparing Scikit-Learn Optimization on White Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciKit-Learn accuracy score on white wine test dataset: 0.5338512763596004\n",
      "Total execution time: 152.18 milliseconds\n"
     ]
    }
   ],
   "source": [
    "#test sklearn on white wine dataset\n",
    "t0 = time.time()\n",
    "skl_logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "skl_logreg.fit(X_train_w, y_train_w)\n",
    "skl_yhat = skl_logreg.predict(X_test_w)\n",
    "t1 = time.time()\n",
    "\n",
    "print('SciKit-Learn accuracy score on white wine test dataset:', \n",
    "      accuracy_score(y_test_w, skl_yhat))\n",
    "print('Total execution time: %.2f milliseconds' % ((t1-t0) * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment\n",
    "\n",
    "- Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__ why our model sucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work: \n",
    "\n",
    "- You have free reign to provide additional analyses. __One idea:__ Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. \n",
    "- __Required for 7000 level students:__ Choose ONE of the following:\n",
    "    1. __Option One:__ Implement an optimization technique for logistic regression using __mean square error__ as your objective function (instead of maximum likelihood). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, which process do you prefer: maximum likelihood OR minimum mean-squared error? \n",
    "    2. __Option Two:__ Implement the BFGS algorithm from scratch to optimize logistic regression. That is, use BFGS without the use of an external package (for example, do not use SciPy). Compare your performance accuracy and runtime to the BFGS implementation in SciPy (that we used in lecture). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] UCI Machine Learning Repository. Wine Quality Dataset. https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "\n",
    "[2] Lab 1: Exploring Table Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
