{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Two: Exploring Image Data\n",
    "\n",
    "#### *Harrison Noble, Henry Lambson*\n",
    "\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Overview of the Dataset\n",
    "\n",
    "The dataset we selected contains pictures of faces that are either real, or have been edited. The edited photos have been categorized into easy, mid, and hard in terms of the difficulty to determine that they are fake. The dataset made it clear that these groupings are subjective and as such will not be used as explicit categories. Instead we will just be focusing on whether the image is real or fake. There are 960 fake images and 1081 real images in the dataset. \n",
    "\n",
    "### 1.2 Purpose of the Data\n",
    "\n",
    "This data was gathered in 2019 and according to reference 1, it was gathered for the purpose of training an algorithm to distinguish fake or edited images of faces from real, unedited ones. \n",
    "\n",
    "### 1.3 Prediction Task for Dataset\n",
    "\n",
    "To determine whether a photo on social media has been edited or doctored in some way. This could be useful to find and dispose of bot or fake accounts.  \n",
    "\n",
    "### 1.4 Data Importance\n",
    "\n",
    "This data is important because it can help stop the spread of misleading images such as scammers using fake images for their accounts, or a deep-fake generated incriminating or slandering image of someone. Social media has become such a large part of society so an algorithm that helps detect doctored photos could be useful in a moderating capacity, whether that be flagging images that are detected to be fake, or banning the accounts that use the photos. Since the dataset is only made up of faces, these would be the only kinds of images that could be detected by the algorithm.       \n",
    "\n",
    "### 1.5 Prediction Algorithm Performace to be Considered Useful\n",
    "\n",
    "We believe that for this algorithm to be useful, it would need to be quite accurate in order to prevent unjust bans or flags on social media images. To that end, we would want to reduce the amount of false-positives so that regular users are not affected. Given the number of images posted to social media in a day, we would want our algorithm to have a success rate somewhere in the range of 95-98%. Instead of outright banning users, the algorithm would flag an account to be reviewed manually if multiple infractions are detected.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Read in and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fake images array: (0,)\n",
      "Shape of real images array: (0,)\n",
      "Shape of combined images array: (0,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#unzip data (only do once, uncomment to unzip data)\n",
    "# with zipfile.ZipFile('dataset.zip', 'r') as zipf:\n",
    "#     zipf.extractall('./data')\n",
    "\n",
    "fake_face_path = 'data/real_and_fake_face/training_fake/*'\n",
    "real_face_path = 'data/real_and_fake_face/training_real/*'\n",
    "\n",
    "def load_data(path, data_type):\n",
    "    files = glob.glob(path)\n",
    "    \n",
    "    img_list = []\n",
    "    f_name = []\n",
    "    for file in files:\n",
    "        #create image, resize to 200x200, convert to grayscale\n",
    "        img = Image.open(file)\n",
    "        img = img.resize((200, 200))\n",
    "        img = img.convert('L')\n",
    "        #convert image to numpy array and flatten\n",
    "        data = np.asarray(img)\n",
    "        data = data.flatten()\n",
    "        #add image to list of images\n",
    "        img_list.append(data)\n",
    "        \n",
    "        _, fname = os.path.split(file)\n",
    "        f_name.append(fname)\n",
    "        \n",
    "    return np.asarray(img_list), f_name\n",
    "\n",
    "#load fake images (0 signifies fake)\n",
    "fake_list, f_names_fake = load_data(fake_face_path, 0)\n",
    "#load real images (1 signifies real)\n",
    "real_list, f_names_real = load_data(real_face_path, 1)\n",
    "\n",
    "print('Shape of fake images array:', fake_list.shape)\n",
    "print('Shape of real images array:', real_list.shape)\n",
    "\n",
    "data = np.concatenate((fake_list, real_list), axis=0)\n",
    "names = f_names_fake + f_names_real\n",
    "\n",
    "print('Shape of combined images array:', data.shape)\n",
    "print(len(names))\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['data/real_and_fake_face/training_fake/mid_243_0011', 'jpg'], ['data/real_and_fake_face/training_fake/hard_23_1110', 'jpg'], ['data/real_and_fake_face/training_fake/hard_185_1100', 'jpg'], ['data/real_and_fake_face/training_fake/mid_326_1111', 'jpg'], ['data/real_and_fake_face/training_fake/mid_343_1111', 'jpg']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Reduction\n",
    "\n",
    "### 3.1 Linear Dimensionality Reduction Using Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function taken from your \"04. Dimension Reduction and Images\" notebook\n",
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Bar, Line\n",
    "    from plotly.graph_objs import Scatter, Layout\n",
    "    from plotly.graph_objs.scatter import Marker\n",
    "    from plotly.graph_objs.layout import XAxis, YAxis\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Linear Dimensionality Reduction Using Randomized Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparing PCA and Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analysis of Feature Extraction for Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work: ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Real and Fake Face Dataset. https://www.kaggle.com/ciplab/real-and-fake-face-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
